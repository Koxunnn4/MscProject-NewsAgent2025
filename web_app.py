from fastapi import FastAPI, Request, Form, APIRouter
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import uvicorn
from news_search import NewsSearchEngine
import logging
from typing import Optional, List
import json
import os
import sys
from collections import Counter
import re
from datetime import datetime, timedelta

# --- Start of web_analyzer integration ---

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥åˆ†æå™¨æ¨¡å—
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

try:
    from src.crypto_analysis.similarity_analyzer import SimilarityAnalyzer
    from src.crypto_analysis.model_loader import get_spacy_model
    ANALYZER_AVAILABLE = True
except ImportError as e:
    logging.warning(f"æ— æ³•å¯¼å…¥åˆ†æå™¨æ¨¡å—ï¼Œåˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨: {e}")
    ANALYZER_AVAILABLE = False
    SimilarityAnalyzer = None
    get_spacy_model = None

# é¢‘é“æ˜ å°„
CHANNEL_MAP = {
    "1": ("-1001387109317", "@theblockbeats"),
    "2": ("-1001735732363", "@TechFlowDaily"),
    "3": ("-1002395608815", "@news6551"),
    "4": ("-1002117032512", "@MMSnews"),
}

class WebSimilarityAnalyzer:
    """
    Web ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦åˆ†æå™¨
    åŸºäº SimilarityAnalyzer çš„åŒ…è£…ï¼Œæä¾› Web API æ‰€éœ€çš„æ¥å£
    """
    def __init__(self, db_path=r"src/crawler/crpyto_news/stream.db", table="messages",
                 keyword_column="keywords", currency_column="industry",
                 min_count=5, top_n=100):
        self.db_path = db_path
        self.table = table
        self.keyword_column = keyword_column
        self.currency_column = currency_column
        self.min_count = min_count
        self.top_n = top_n

        if ANALYZER_AVAILABLE:
            try:
                self.analyzer = SimilarityAnalyzer(
                    db_path=db_path, table=table, keyword_column=keyword_column,
                    currency_column=currency_column, min_count=min_count, top_n=top_n
                )
                logging.info(f"âœ“ åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ | æ•°æ®åº“: {db_path}")
            except Exception as e:
                logging.error(f"âœ— åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.analyzer = None
        else:
            self.analyzer = None

    def get_total_rows(self, channel_ids=None, time_range=None):
        if not self.analyzer: return 0
        try:
            return self.analyzer.get_total_rows(channel_ids=channel_ids, time_range=time_range)
        except Exception as e:
            logging.error(f"è·å–æ€»è¡Œæ•°å¤±è´¥: {e}")
            return 0

    def fetch_column_data(self, column, channel_ids=None, time_range=None):
        if not self.analyzer: return []
        try:
            return self.analyzer.fetch_column_data(column=column, channel_ids=channel_ids, time_range=time_range)
        except Exception as e:
            logging.error(f"è¯»å–åˆ—æ•°æ®å¤±è´¥: {e}")
            return []

    def count_items_with_occurrence(self, rows, case_insensitive=True):
        if not self.analyzer: return Counter(), Counter()
        try:
            return self.analyzer.count_items_with_occurrence(rows=rows, case_insensitive=case_insensitive)
        except Exception as e:
            logging.error(f"ç»Ÿè®¡é¡¹ç›®å‡ºç°æ¬¡æ•°å¤±è´¥: {e}")
            return Counter(), Counter()

    def calculate_similarity(self, keyword_counter, limit=None):
        if not self.analyzer: return []
        try:
            pairs = self.analyzer.calculate_similarity(keyword_counter)
            limit = limit or self.top_n
            return pairs[:limit]
        except Exception as e:
            logging.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return []

    def query_keyword_similarity(self, input_keyword, keyword_counter):
        if not self.analyzer: return False, []
        try:
            return self.analyzer.query_keyword_similarity(
                input_keyword=input_keyword, keyword_counter=keyword_counter, top_n=10
            )
        except Exception as e:
            logging.error(f"æŸ¥è¯¢ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return False, []

# åˆå§‹åŒ–åˆ†æå™¨å®ä¾‹
web_analyzer = WebSimilarityAnalyzer()

# --- End of web_analyzer integration ---

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="Web3æ–°é—»åˆ†æå¹³å°", description="ä¸€ä¸ªé›†æ–°é—»æœç´¢ä¸å…³é”®è¯åˆ†æäºä¸€ä½“çš„å¹³å°")

# æŒ‚è½½ static æ–‡ä»¶å¤¹
app.mount("/static", StaticFiles(directory="static"), name="static")

# é…ç½®æ¨¡æ¿ç›®å½•
templates = Jinja2Templates(directory="templates_UI")

# åˆå§‹åŒ–æœç´¢å¼•æ“
search_engine = NewsSearchEngine()

# ä¸»é¡µ
@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    return templates.TemplateResponse("home.html", {"request": request})

# æ–°é—»æœç´¢é¡µé¢
@app.get("/search", response_class=HTMLResponse)
async def search_page(request: Request, keyword: Optional[str] = None, top_k: int = 5):
    if keyword:
        return await search_news_get(request, keyword, top_k)
    
    top_keywords_counts = search_engine.get_top_keywords_with_counts(20)
    default_keyword = top_keywords_counts[0]["keyword"] if top_keywords_counts else ""
    
    if default_keyword:
        return await search_news_get(request, default_keyword, top_k)
    
    return templates.TemplateResponse("search_results.html", {
        "request": request, "keyword": "æ— ", "results": [], "top_keywords_counts": [],
        "max_count": 1, "min_count": 0, "total_results": 0, 
        "trend_labels": "[]", "trend_counts": "[]"
    })

# å…³é”®è¯åˆ†æå™¨é¡µé¢
@app.get("/analyzer", response_class=HTMLResponse)
async def analyzer_page(request: Request):
    return templates.TemplateResponse("analyzer.html", {"request": request})


# --- API for News Search ---

@app.get("/search_action", response_class=HTMLResponse)
async def search_news_get(request: Request, keyword: str, top_k: int = 5):
    try:
        results = search_engine.search_by_keyword(keyword, top_k)
        for news in results:
            news['summary'] = search_engine.generate_summary(news['original_text'])
        top_keywords_counts = search_engine.get_top_keywords_with_counts(20)
        max_count = max([item["count"] for item in top_keywords_counts]) if top_keywords_counts else 1
        min_count = min([item["count"] for item in top_keywords_counts]) if top_keywords_counts else 0
        trend_day = search_engine.get_keyword_trend(keyword, granularity="day")
        trend_labels = json.dumps([p['time'] for p in trend_day], ensure_ascii=False)
        trend_counts = json.dumps([p['count'] for p in trend_day], ensure_ascii=False)
        return templates.TemplateResponse("search_results.html", {
            "request": request,
            "keyword": keyword,
            "results": results,
            "top_keywords_counts": top_keywords_counts,
            "max_count": max_count,
            "min_count": min_count,
            "total_results": len(results),
            "trend_labels": trend_labels,
            "trend_counts": trend_counts,
        })
    except Exception as e:
        logger.error(f"æœç´¢å‡ºé”™: {e}")
        return templates.TemplateResponse("error.html", {
            "request": request,
            "error_message": f"æœç´¢å‡ºé”™: {str(e)}"
        })

@app.post("/search_action", response_class=HTMLResponse)
async def search_news_post(request: Request, keyword: str = Form(...), top_k: int = Form(5)):
    return await search_news_get(request, keyword, top_k)

# --- APIs for Keyword Analyzer ---

analyzer_router = APIRouter(prefix="/api")

@analyzer_router.get("/channels")
async def get_channels():
    """è·å–å¯ç”¨é¢‘é“åˆ—è¡¨"""
    channels = [{'id': k, 'name': v[1], 'channel_id': v[0]} for k, v in CHANNEL_MAP.items()]
    return JSONResponse(content={'channels': channels})

@analyzer_router.post("/analyze")
async def analyze_data(request: Request):
    """åˆ†ææ•°æ®çš„ä¸»è¦æ¥å£"""
    try:
        data = await request.json()
        channel_ids = data.get('channel_ids', [])
        time_range_str = data.get('time_range')

        logger.info(f"\nğŸ“Š å¼€å§‹åˆ†æ...")
        logger.info(f"   é¢‘é“ ID: {channel_ids}")
        logger.info(f"   æ—¶é—´èŒƒå›´: {time_range_str}")

        total_rows = web_analyzer.get_total_rows(channel_ids or None, time_range_str)
        logger.info(f"âœ“ æ€»è¡Œæ•°: {total_rows}")

        keyword_rows = web_analyzer.fetch_column_data(web_analyzer.keyword_column, channel_ids or None, time_range_str)
        logger.info(f"âœ“ è¯»å–å…³é”®è¯è¡Œæ•°: {len(keyword_rows)}")
        keyword_counter, keyword_occurrence = web_analyzer.count_items_with_occurrence(keyword_rows)
        logger.info(f"âœ“ å…³é”®è¯ç§ç±»: {len(keyword_counter)}")

        currency_rows = web_analyzer.fetch_column_data(web_analyzer.currency_column, channel_ids or None, time_range_str)
        logger.info(f"âœ“ è¯»å–å¸ç§è¡Œæ•°: {len(currency_rows)}")
        currency_counter, currency_occurrence = web_analyzer.count_items_with_occurrence(currency_rows)
        logger.info(f"âœ“ å¸ç§ç§ç±»: {len(currency_counter)}")

        similarity_pairs = web_analyzer.calculate_similarity(keyword_counter, limit=50)
        similarity_results = [
            {'word1': a, 'count1': ca, 'word2': b, 'count2': cb, 'similarity': round(s, 4)}
            for a, ca, b, cb, s in similarity_pairs
        ]

        keyword_stats = []
        for word, count in keyword_counter.most_common():
            occur_count = keyword_occurrence[word]
            ratio = (occur_count / total_rows * 100) if total_rows > 0 else 0
            keyword_stats.append({'word': word, 'count': count, 'occur_count': occur_count, 'ratio': round(ratio, 2)})

        currency_stats = []
        for word, count in currency_counter.most_common():
            occur_count = currency_occurrence[word]
            ratio = (occur_count / total_rows * 100) if total_rows > 0 else 0
            currency_stats.append({'word': word, 'count': count, 'occur_count': occur_count, 'ratio': round(ratio, 2)})

        logger.info("âœ… åˆ†æå®Œæˆ\n")
        return JSONResponse(content={
            'success': True, 'total_rows': total_rows, 'keyword_stats': keyword_stats,
            'currency_stats': currency_stats, 'similarity_results': similarity_results,
            'keyword_total': len(keyword_counter), 'currency_total': len(currency_counter)
        })
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={'success': False, 'error': str(e)})

@analyzer_router.post("/query-keyword")
async def query_keyword_similarity(request: Request):
    """æŸ¥è¯¢å…³é”®è¯ç›¸ä¼¼åº¦"""
    try:
        data = await request.json()
        keyword = data.get('keyword', '').strip()
        channel_ids = data.get('channel_ids', [])
        time_range = data.get('time_range')

        logger.info(f"\nğŸ” æŸ¥è¯¢è¯·æ±‚: '{keyword}'")
        if not keyword:
            return JSONResponse(status_code=400, content={'success': False, 'error': 'è¯·è¾“å…¥å…³é”®è¯'})

        keyword_rows = web_analyzer.fetch_column_data(web_analyzer.keyword_column, channel_ids or None, time_range)
        keyword_counter, _ = web_analyzer.count_items_with_occurrence(keyword_rows)
        
        exists, top_similar = web_analyzer.query_keyword_similarity(keyword, keyword_counter)
        similar_results = [{'word': word, 'count': count, 'similarity': round(similarity, 4)} for word, count, similarity in top_similar]

        logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(similar_results)} ä¸ªç›¸ä¼¼è¯\n")
        return JSONResponse(content={'success': True, 'keyword': keyword, 'exists': exists, 'similar_words': similar_results})
    except Exception as e:
        logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={'success': False, 'error': str(e)})

app.include_router(analyzer_router)

# --- Original API endpoints (can be kept or refactored) ---


if __name__ == "__main__":
    uvicorn.run("web_app:app", host="127.0.0.1", port=8001, reload=True)