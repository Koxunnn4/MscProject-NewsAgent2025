"""
ç›¸ä¼¼åº¦åˆ†æå·¥å…· - Web å‰ç«¯ç‰ˆæœ¬
ä½¿ç”¨ Flask æä¾›åç«¯ APIï¼Œå‰ç«¯é€šè¿‡ HTML/JS å‘ˆç°
å·²é€‚é…é‡æ„åçš„åˆ†ææ¨¡å—ï¼šä½¿ç”¨ SimilarityAnalyzer å’Œ model_loader
"""
import sqlite3
import re
import argparse
import json
from datetime import datetime, timedelta, timezone
from collections import Counter
from flask import Flask, render_template, request, jsonify, send_from_directory
from pathlib import Path
import os
import sys
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# å¯¼å…¥æ–°çš„åˆ†ææ¨¡å—
from src.crypto_analysis.similarity_analyzer import SimilarityAnalyzer
from src.crypto_analysis.model_loader import get_spacy_model

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ======= é»˜è®¤é…ç½®å‚æ•° =======
DEFAULT_DB_PATH = r"E:\msc_proj\cjyBranch\MscProject-NewsAgent2025-chenjingyin\src\crawler\crpyto_news\stream.db"
DEFAULT_TABLE = "messages"
DEFAULT_KEYWORD_COLUMN = "keywords"
DEFAULT_CURRENCY_COLUMN = "industry"
DEFAULT_MIN_COUNT = 5
DEFAULT_TOP_N = 100

SPLIT_RE = re.compile(r"[,ï¼Œ]+")

# é¢‘é“æ˜ å°„
CHANNEL_MAP = {
    "1": ("-1001387109317", "@theblockbeats"),
    "2": ("-1001735732363", "@TechFlowDaily"),
    "3": ("-1002395608815", "@news6551"),
    "4": ("-1002117032512", "@MMSnews"),
}

app = Flask(__name__, template_folder='templates', static_folder='static')

class WebSimilarityAnalyzer:
    """
    Web ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦åˆ†æå™¨
    åŸºäº SimilarityAnalyzer çš„åŒ…è£…ï¼Œæä¾› Web API æ‰€éœ€çš„æ¥å£
    """

    def __init__(self, db_path=DEFAULT_DB_PATH, table=DEFAULT_TABLE,
                 keyword_column=DEFAULT_KEYWORD_COLUMN,
                 currency_column=DEFAULT_CURRENCY_COLUMN,
                 min_count=DEFAULT_MIN_COUNT, top_n=DEFAULT_TOP_N):
        """åˆå§‹åŒ–åˆ†æå™¨ï¼Œåˆ›å»º SimilarityAnalyzer å®ä¾‹"""
        self.db_path = db_path
        self.table = table
        self.keyword_column = keyword_column
        self.currency_column = currency_column
        self.min_count = min_count
        self.top_n = top_n

        # ä½¿ç”¨æ–°çš„åˆ†ææ¨¡å—
        try:
            self.analyzer = SimilarityAnalyzer(
                db_path=db_path,
                table=table,
                keyword_column=keyword_column,
                currency_column=currency_column,
                min_count=min_count,
                top_n=top_n
            )
            logger.info(f"âœ“ åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ | æ•°æ®åº“: {db_path}")
        except Exception as e:
            logger.error(f"âœ— åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.analyzer = None

    def get_total_rows(self, channel_ids=None, time_range=None):
        """è·å–æ•°æ®åº“æ€»è¡Œæ•°"""
        try:
            return self.analyzer.get_total_rows(channel_ids=channel_ids, time_range=time_range)
        except Exception as e:
            logger.error(f"è·å–æ€»è¡Œæ•°å¤±è´¥: {e}")
            return 0

    def fetch_column_data(self, column, channel_ids=None, time_range=None):
        """ä»æ•°æ®åº“è¯»å–æŒ‡å®šåˆ—æ•°æ®"""
        try:
            return self.analyzer.fetch_column_data(column=column, channel_ids=channel_ids, time_range=time_range)
        except Exception as e:
            logger.error(f"è¯»å–åˆ—æ•°æ®å¤±è´¥: {e}")
            return []

    def count_items_with_occurrence(self, rows, case_insensitive=True):
        """ç»Ÿè®¡åˆ†éš”å­—ç¬¦ä¸²ä¸­å„é¡¹çš„å‡ºç°æ¬¡æ•°"""
        try:
            return self.analyzer.count_items_with_occurrence(rows=rows, case_insensitive=case_insensitive)
        except Exception as e:
            logger.error(f"ç»Ÿè®¡é¡¹ç›®å‡ºç°æ¬¡æ•°å¤±è´¥: {e}")
            return Counter(), Counter()

    def calculate_similarity(self, keyword_counter, limit=None):
        """
        è®¡ç®—å…³é”®è¯ç›¸ä¼¼åº¦

        Args:
            keyword_counter: å…³é”®è¯è®¡æ•°å™¨
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            ç›¸ä¼¼åº¦å¯¹åˆ—è¡¨
        """
        try:
            pairs = self.analyzer.calculate_similarity(keyword_counter)
            limit = limit or self.top_n
            return pairs[:limit]
        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return []

    def query_keyword_similarity(self, input_keyword, keyword_counter):
        """
        æŸ¥è¯¢å…³é”®è¯ç›¸ä¼¼åº¦

        Args:
            input_keyword: è¾“å…¥çš„å…³é”®è¯
            keyword_counter: å…³é”®è¯è®¡æ•°å™¨

        Returns:
            (exists, similar_words) å…ƒç»„
        """
        try:
            return self.analyzer.query_keyword_similarity(
                input_keyword=input_keyword,
                keyword_counter=keyword_counter,
                top_n=10
            )
        except Exception as e:
            logger.error(f"æŸ¥è¯¢ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return False, []


# åˆå§‹åŒ–åˆ†æå™¨
analyzer = WebSimilarityAnalyzer()


@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')


@app.route('/api/analyze', methods=['POST'])
def analyze():
    """åˆ†ææ•°æ®çš„ä¸»è¦æ¥å£"""
    try:
        data = request.json
        channel_ids = data.get('channel_ids', [])
        time_range = data.get('time_range')

        print(f"\nğŸ“Š å¼€å§‹åˆ†æ...")
        print(f"   é¢‘é“ ID: {channel_ids}")
        print(f"   æ—¶é—´èŒƒå›´: {time_range}")

        # è·å–æ•°æ®åº“æ€»è¡Œæ•°
        total_rows = analyzer.get_total_rows(channel_ids or None, time_range)
        print(f"âœ“ æ€»è¡Œæ•°: {total_rows}")

        # è·å–å…³é”®è¯æ•°æ®
        print("ğŸ“¥ æ­£åœ¨è¯»å–å…³é”®è¯æ•°æ®...")
        keyword_rows = analyzer.fetch_column_data(
            analyzer.keyword_column,
            channel_ids or None,
            time_range
        )
        print(f"âœ“ è¯»å–å…³é”®è¯è¡Œæ•°: {len(keyword_rows)}")
        keyword_counter, keyword_occurrence = analyzer.count_items_with_occurrence(keyword_rows)
        print(f"âœ“ å…³é”®è¯ç§ç±»: {len(keyword_counter)}")

        # è·å–å¸ç§æ•°æ®
        print("ğŸ“¥ æ­£åœ¨è¯»å–å¸ç§æ•°æ®...")
        currency_rows = analyzer.fetch_column_data(
            analyzer.currency_column,
            channel_ids or None,
            time_range
        )
        print(f"âœ“ è¯»å–å¸ç§è¡Œæ•°: {len(currency_rows)}")
        currency_counter, currency_occurrence = analyzer.count_items_with_occurrence(currency_rows)
        print(f"âœ“ å¸ç§ç§ç±»: {len(currency_counter)}")

        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆè¿”å› top 50ï¼‰
        print("ğŸ”— è®¡ç®—ç›¸ä¼¼åº¦...")
        similarity_pairs = analyzer.calculate_similarity(keyword_counter, limit=50)
        similarity_results = [
            {
                'word1': a,
                'count1': ca,
                'word2': b,
                'count2': cb,
                'similarity': round(s, 4)
            }
            for a, ca, b, cb, s in similarity_pairs
        ]

        # æ„å»ºå…³é”®è¯ç»Ÿè®¡ç»“æœï¼ˆè¿”å›å…¨éƒ¨æ•°æ®ï¼Œç”±å‰ç«¯åˆ†é¡µï¼‰
        print("ğŸ“ æ„å»ºå…³é”®è¯ç»Ÿè®¡...")
        keyword_stats = []
        for word, count in keyword_counter.most_common():
            occur_count = keyword_occurrence[word]
            ratio = (occur_count / total_rows * 100) if total_rows > 0 else 0
            keyword_stats.append({
                'word': word,
                'count': count,
                'occur_count': occur_count,
                'ratio': round(ratio, 2)
            })
        print(f"âœ“ å…³é”®è¯ç»Ÿè®¡æ¡ç›®: {len(keyword_stats)}")

        # æ„å»ºå¸ç§ç»Ÿè®¡ç»“æœ
        print("ğŸ’° æ„å»ºå¸ç§ç»Ÿè®¡...")
        currency_stats = []
        for word, count in currency_counter.most_common():
            occur_count = currency_occurrence[word]
            ratio = (occur_count / total_rows * 100) if total_rows > 0 else 0
            currency_stats.append({
                'word': word,
                'count': count,
                'occur_count': occur_count,
                'ratio': round(ratio, 2)
            })
        print(f"âœ“ å¸ç§ç»Ÿè®¡æ¡ç›®: {len(currency_stats)}")

        print("âœ… åˆ†æå®Œæˆ\n")
        return jsonify({
            'success': True,
            'total_rows': total_rows,
            'keyword_stats': keyword_stats,
            'currency_stats': currency_stats,
            'similarity_results': similarity_results,
            'keyword_total': len(keyword_counter),
            'currency_total': len(currency_counter)
        })

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/query-keyword', methods=['POST'])
def query_keyword():
    """æŸ¥è¯¢å…³é”®è¯ç›¸ä¼¼åº¦"""
    try:
        data = request.json
        keyword = data.get('keyword', '').strip()
        channel_ids = data.get('channel_ids', [])
        time_range = data.get('time_range')

        print(f"\nğŸ” æŸ¥è¯¢è¯·æ±‚: '{keyword}'")

        if not keyword:
            print("âš ï¸ å…³é”®è¯ä¸ºç©º")
            return jsonify({'success': False, 'error': 'è¯·è¾“å…¥å…³é”®è¯'}), 400

        # è·å–èŒƒå›´å†…çš„å…³é”®è¯ç»Ÿè®¡
        print("ğŸ“¥ æ­£åœ¨è¯»å–å…³é”®è¯æ•°æ®...")
        keyword_rows = analyzer.fetch_column_data(
            analyzer.keyword_column,
            channel_ids or None,
            time_range
        )
        print(f"âœ“ è¯»å–å…³é”®è¯è¡Œæ•°: {len(keyword_rows)}")
        keyword_counter, _ = analyzer.count_items_with_occurrence(keyword_rows)
        print(f"âœ“ å…³é”®è¯ç§ç±»: {len(keyword_counter)}")

        # æŸ¥è¯¢ç›¸ä¼¼åº¦
        exists, top_similar = analyzer.query_keyword_similarity(keyword, keyword_counter)

        similar_results = [
            {
                'word': word,
                'count': count,
                'similarity': round(similarity, 4)
            }
            for word, count, similarity in top_similar
        ]

        print(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(similar_results)} ä¸ªç›¸ä¼¼è¯\n")
        return jsonify({
            'success': True,
            'keyword': keyword,
            'exists': exists,
            'similar_words': similar_results
        })

    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500
@app.route('/api/channels', methods=['GET'])
def get_channels():
    """è·å–å¯ç”¨é¢‘é“åˆ—è¡¨"""
    channels = [
        {'id': k, 'name': v[1], 'channel_id': v[0]}
        for k, v in CHANNEL_MAP.items()
    ]
    return jsonify({'channels': channels})


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Web ç‰ˆæœ¬ç›¸ä¼¼åº¦åˆ†æå·¥å…·")
    parser.add_argument('--db-path', type=str, default=DEFAULT_DB_PATH,
                       help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--table', type=str, default=DEFAULT_TABLE,
                       help="æ•°æ®åº“è¡¨å")
    parser.add_argument('--port', type=int, default=5000,
                       help="Flask æœåŠ¡ç«¯å£")
    parser.add_argument('--host', type=str, default='127.0.0.1',
                       help="Flask æœåŠ¡ä¸»æœº")
    return parser.parse_args()


if __name__ == '__main__':
    args = parse_arguments()
    analyzer.db_path = args.db_path
    analyzer.table = args.table

    print(f"\n{'='*80}")
    print("ç›¸ä¼¼åº¦åˆ†æå·¥å…· - Web ç‰ˆæœ¬")
    print(f"{'='*80}")
    print(f"æ•°æ®åº“: {args.db_path}")
    print(f"è¡¨å: {args.table}")
    print(f"æœåŠ¡å™¨: http://{args.host}:{args.port}")
    print(f"{'='*80}\n")

    app.run(host=args.host, port=args.port, debug=True)
