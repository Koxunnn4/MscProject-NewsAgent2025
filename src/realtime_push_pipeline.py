"""
å®æ—¶æ¨é€Pipeline
æ•´åˆçˆ¬è™«ã€å…³é”®è¯æå–ã€è®¢é˜…åŒ¹é…å’Œæ¨é€çš„å®Œæ•´æµç¨‹
"""
import os
import sys
import asyncio
from typing import Dict, List, Optional
from datetime import datetime
import logging

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

from src.unified_news_interface import get_unified_news_interface
from src.keyword_matching import get_keyword_matcher
from src.push_system.push_manager import get_push_manager
from src.database.db_manager import get_db_manager
from src.trend_analysis.trend_analyzer import get_trend_analyzer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RealtimePushPipeline:
    """å®æ—¶æ¨é€Pipeline"""
    
    def __init__(self, db_path: str = None):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            db_path: å®æ—¶æ•°æ®åº“è·¯å¾„ï¼ˆå¦‚æœä½¿ç”¨æ–°è€åˆ†ç¦»ç­–ç•¥ï¼‰
        """
        self.news_interface = get_unified_news_interface()
        self.keyword_matcher = get_keyword_matcher()
        self.push_manager = get_push_manager()
        self.db = get_db_manager()
        self.trend_analyzer = get_trend_analyzer()
        
        # ä½¿ç”¨å®æ—¶æ•°æ®åº“æˆ–å†å²æ•°æ®åº“
        self.db_path = db_path or self.db.history_db_path
        
        # æ–°é—»é˜Ÿåˆ—
        self.news_queue = asyncio.Queue()
        
        # æ¨é€é¢‘ç‡é™åˆ¶ï¼ˆæ¯ä¸ªç”¨æˆ·æ¯å°æ—¶æœ€å¤šæ¨é€ï¼‰
        self.push_frequency_limit = {}
        self.push_limit_per_hour = 10
        
        logger.info(f"å®æ—¶æ¨é€Pipelineåˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®åº“: {self.db_path}")
    
    async def on_news_received(self, news_data: Dict, source_type: str = 'crypto'):
        """
        å½“çˆ¬è™«æŠ“å–åˆ°æ–°æ–°é—»æ—¶çš„å›è°ƒï¼ˆPipelineå…¥å£ï¼‰
        
        Args:
            news_data: æ–°é—»æ•°æ®
            source_type: 'crypto' æˆ– 'hkstock'
        """
        logger.info(f"æ”¶åˆ°æ–°æ–°é—» [{source_type}]: {news_data.get('text', '')[:50]}...")
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆå¼‚æ­¥å¤„ç†ï¼‰
        await self.news_queue.put({
            'news_data': news_data,
            'source_type': source_type,
            'received_at': datetime.now().isoformat()
        })
    
    async def process_news_pipeline(self):
        """æ–°é—»å¤„ç†Pipelineä¸»å¾ªç¯"""
        logger.info("å¯åŠ¨æ–°é—»å¤„ç†Pipeline...")
        
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–æ–°é—»
                item = await self.news_queue.get()
                
                news_data = item['news_data']
                source_type = item['source_type']
                
                logger.info(f"å¼€å§‹å¤„ç†æ–°é—» [ID: {news_data.get('id')}]")
                
                # Step 1: ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“
                news_id = await self._save_news(news_data, source_type)
                if not news_id:
                    logger.error("ä¿å­˜æ–°é—»å¤±è´¥ï¼Œè·³è¿‡")
                    continue
                
                # Step 2: æå–å…³é”®è¯
                keywords = await self._extract_keywords(news_id, news_data['text'])
                if not keywords:
                    logger.warning(f"æœªæå–åˆ°å…³é”®è¯ [ID: {news_id}]")
                
                # Step 3: åŒ¹é…è®¢é˜…ç”¨æˆ·
                matched_subscriptions = await self._match_subscriptions(
                    news_data['text'], keywords
                )
                
                # Step 4: æ¨é€ç»™è®¢é˜…ç”¨æˆ·
                if matched_subscriptions:
                    logger.info(f"åŒ¹é…åˆ° {len(matched_subscriptions)} ä¸ªè®¢é˜…")
                    await self._push_to_subscribers(
                        news_id, news_data, matched_subscriptions
                    )
                else:
                    logger.debug(f"æ— åŒ¹é…è®¢é˜… [ID: {news_id}]")
                
                # Step 5: æ›´æ–°çƒ­åº¦ç»Ÿè®¡ï¼ˆå¼‚æ­¥ï¼‰
                asyncio.create_task(
                    self._update_trend_stats(keywords, news_data.get('date'))
                )
                
                logger.info(f"æ–°é—»å¤„ç†å®Œæˆ [ID: {news_id}]")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–°é—»æ—¶å‡ºé”™: {e}", exc_info=True)
                await asyncio.sleep(1)  # å‡ºé”™åçŸ­æš‚ç­‰å¾…
    
    async def _save_news(self, news_data: Dict, source_type: str) -> Optional[int]:
        """
        ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“
        
        Args:
            news_data: æ–°é—»æ•°æ®
            source_type: æ–°é—»æºç±»å‹
            
        Returns:
            æ–°é—»IDï¼Œå¤±è´¥è¿”å›None
        """
        try:
            news_id = self.news_interface.save_news(
                news_data, source_type, self.db_path
            )
            logger.debug(f"æ–°é—»å·²ä¿å­˜ [ID: {news_id}]")
            return news_id
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°é—»å¤±è´¥: {e}", exc_info=True)
            return None
    
    async def _extract_keywords(self, news_id: int, text: str) -> List[tuple]:
        """
        æå–å¹¶ä¿å­˜å…³é”®è¯
        
        Args:
            news_id: æ–°é—»ID
            text: æ–°é—»æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨ [(keyword, weight), ...]
        """
        try:
            keywords = self.news_interface.extract_keywords(text, top_n=10)
            
            if keywords:
                # ä¿å­˜åˆ°æ•°æ®åº“
                self.db.save_news_keywords(news_id, keywords)
                logger.debug(f"å…³é”®è¯å·²ä¿å­˜ [ID: {news_id}]: {[kw for kw, _ in keywords]}")
            
            return keywords
        except Exception as e:
            logger.error(f"æå–å…³é”®è¯å¤±è´¥: {e}", exc_info=True)
            return []
    
    async def _match_subscriptions(self, news_text: str,
                                   keywords: List[tuple]) -> List[Dict]:
        """
        åŒ¹é…è®¢é˜…ç”¨æˆ·
        
        Args:
            news_text: æ–°é—»æ–‡æœ¬
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            åŒ¹é…çš„è®¢é˜…åˆ—è¡¨
        """
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒè®¢é˜…
            query = """
            SELECT id, user_id, keyword, telegram_chat_id
            FROM subscriptions
            WHERE is_active = 1
            """
            all_subscriptions = self.db.execute_query(query, db_path=self.db_path)
            
            if not all_subscriptions:
                return []
            
            matched = []
            
            for sub in all_subscriptions:
                # å…³é”®è¯åŒ¹é…
                match_result = self.keyword_matcher.match_keyword(
                    news_text, sub['keyword'], threshold=0.3
                )
                
                if match_result['is_match']:
                    # æ£€æŸ¥æ¨é€é¢‘ç‡é™åˆ¶
                    if self._check_push_frequency(sub['user_id']):
                        matched.append({
                            'subscription_id': sub['id'],
                            'user_id': sub['user_id'],
                            'keyword': sub['keyword'],
                            'telegram_chat_id': sub['telegram_chat_id'],
                            'relevance_score': match_result['relevance_score'],
                            'matched_context': match_result.get('context', ''),
                            'match_method': match_result['match_method']
                        })
            
            return matched
            
        except Exception as e:
            logger.error(f"åŒ¹é…è®¢é˜…å¤±è´¥: {e}", exc_info=True)
            return []
    
    async def _push_to_subscribers(self, news_id: int, news_data: Dict,
                                   subscriptions: List[Dict]):
        """
        æ¨é€ç»™è®¢é˜…ç”¨æˆ·
        
        Args:
            news_id: æ–°é—»ID
            news_data: æ–°é—»æ•°æ®
            subscriptions: è®¢é˜…åˆ—è¡¨
        """
        for sub in subscriptions:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²æ¨é€
                if self.db.check_news_pushed(sub['subscription_id'], news_id):
                    logger.debug(f"æ–°é—»å·²æ¨é€è¿‡ [è®¢é˜…ID: {sub['subscription_id']}]")
                    continue
                
                # æ ¼å¼åŒ–æ¨é€æ¶ˆæ¯
                message = self._format_push_message(
                    news_data,
                    sub['keyword'],
                    sub['relevance_score'],
                    sub.get('matched_context', '')
                )
                
                # å‘é€Telegramæ¶ˆæ¯
                success = await self.push_manager.send_telegram_message(
                    chat_id=sub['telegram_chat_id'],
                    news=news_data,
                    keyword=sub['keyword']
                )
                
                # ä¿å­˜æ¨é€å†å²
                status = 'success' if success else 'failed'
                self.db.save_push_history(
                    sub['subscription_id'],
                    news_id,
                    status
                )
                
                if success:
                    # æ›´æ–°æ¨é€é¢‘ç‡è®°å½•
                    self._record_push(sub['user_id'])
                    logger.info(
                        f"âœ“ æ¨é€æˆåŠŸ [ç”¨æˆ·: {sub['user_id']}, "
                        f"å…³é”®è¯: '{sub['keyword']}', "
                        f"ç›¸å…³æ€§: {sub['relevance_score']:.2f}]"
                    )
                else:
                    logger.warning(f"âœ— æ¨é€å¤±è´¥ [ç”¨æˆ·: {sub['user_id']}]")
                
            except Exception as e:
                logger.error(
                    f"æ¨é€å‡ºé”™ [è®¢é˜…ID: {sub['subscription_id']}]: {e}",
                    exc_info=True
                )
    
    def _format_push_message(self, news_data: Dict, keyword: str,
                            relevance_score: float, context: str = '') -> str:
        """æ ¼å¼åŒ–æ¨é€æ¶ˆæ¯"""
        text = news_data['text']
        title = news_data.get('title', text[:100] + '...')
        date = news_data.get('date', '')
        
        # æˆªå–æ‘˜è¦
        summary = text[:300] + '...' if len(text) > 300 else text
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        try:
            date_obj = datetime.fromisoformat(date.replace('T', ' ').replace('+00:00', ''))
            date_formatted = date_obj.strftime('%Y-%m-%d %H:%M')
        except:
            date_formatted = date
        
        message = f"""
ğŸ”” *å…³é”®è¯æ¨é€ï¼š{keyword}*

ğŸ“° *æ ‡é¢˜ï¼š*
{title}

ğŸ“ *æ‘˜è¦ï¼š*
{summary}

ğŸ“… *æ—¶é—´ï¼š* {date_formatted}
â­ *ç›¸å…³æ€§ï¼š* {relevance_score:.0%}

_æ¥è‡ª æ–°é—»åˆ†æç³»ç»Ÿ_
        """.strip()
        
        return message
    
    def _check_push_frequency(self, user_id: str) -> bool:
        """
        æ£€æŸ¥æ¨é€é¢‘ç‡é™åˆ¶
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            æ˜¯å¦å…è®¸æ¨é€
        """
        now = datetime.now()
        hour_key = now.strftime('%Y-%m-%d-%H')
        
        key = f"{user_id}_{hour_key}"
        
        if key not in self.push_frequency_limit:
            self.push_frequency_limit[key] = 0
        
        if self.push_frequency_limit[key] >= self.push_limit_per_hour:
            logger.warning(
                f"ç”¨æˆ· {user_id} æœ¬å°æ—¶æ¨é€æ¬¡æ•°å·²è¾¾ä¸Šé™ "
                f"({self.push_limit_per_hour})"
            )
            return False
        
        return True
    
    def _record_push(self, user_id: str):
        """è®°å½•æ¨é€æ¬¡æ•°"""
        now = datetime.now()
        hour_key = now.strftime('%Y-%m-%d-%H')
        key = f"{user_id}_{hour_key}"
        
        if key not in self.push_frequency_limit:
            self.push_frequency_limit[key] = 0
        
        self.push_frequency_limit[key] += 1
    
    async def _update_trend_stats(self, keywords: List[tuple], date_str: str):
        """
        æ›´æ–°çƒ­åº¦ç»Ÿè®¡ï¼ˆå¼‚æ­¥ï¼‰
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
        """
        try:
            if not date_str:
                return
            
            # æå–æ—¥æœŸéƒ¨åˆ†
            date = date_str.split('T')[0] if 'T' in date_str else date_str[:10]
            
            for keyword, weight in keywords:
                self.db.save_keyword_trend(keyword, date, 1, weight)
            
            logger.debug(f"çƒ­åº¦ç»Ÿè®¡å·²æ›´æ–° [{len(keywords)} ä¸ªå…³é”®è¯]")
            
        except Exception as e:
            logger.error(f"æ›´æ–°çƒ­åº¦ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
    
    async def run(self):
        """å¯åŠ¨Pipeline"""
        logger.info("=" * 70)
        logger.info("  å®æ—¶æ¨é€Pipelineå¯åŠ¨")
        logger.info("=" * 70)
        logger.info(f"æ•°æ®åº“: {self.db_path}")
        logger.info(f"æ¨é€é¢‘ç‡é™åˆ¶: {self.push_limit_per_hour}/å°æ—¶/ç”¨æˆ·")
        logger.info("")
        
        # å¯åŠ¨æ–°é—»å¤„ç†å¾ªç¯
        await self.process_news_pipeline()


# å•ä¾‹æ¨¡å¼
_pipeline = None

def get_realtime_push_pipeline(db_path: str = None) -> RealtimePushPipeline:
    """è·å–å®æ—¶æ¨é€Pipelineå•ä¾‹"""
    global _pipeline
    if _pipeline is None:
        _pipeline = RealtimePushPipeline(db_path)
    return _pipeline


if __name__ == "__main__":
    # æµ‹è¯•Pipeline
    async def test_pipeline():
        pipeline = get_realtime_push_pipeline()
        
        # æ¨¡æ‹Ÿæ–°é—»æ•°æ®
        test_news = {
            'id': 999999,
            'channel_id': 'test_channel',
            'message_id': 999999,
            'text': 'æ¯”ç‰¹å¸ä»·æ ¼çªç ´$95,000ç¾å…ƒï¼Œåˆ›ä¸‹å†å²æ–°é«˜ã€‚å¸‚åœºåˆ†æå¸ˆè®¤ä¸ºè¿™ä¸æœºæ„æŠ•èµ„è€…å¢åŠ æœ‰å…³ã€‚',
            'date': datetime.now().isoformat(),
            'title': 'æ¯”ç‰¹å¸çªç ´$95,000'
        }
        
        # å‘é€åˆ°Pipeline
        await pipeline.on_news_received(test_news, 'crypto')
        
        # è¿è¡ŒPipelineï¼ˆä¼šæŒç»­è¿è¡Œï¼‰
        await pipeline.run()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_pipeline())

