# æ¸¯è‚¡æ–°é—»çˆ¬è™«æ¨¡å—

ä» AAStocks ç½‘ç«™çˆ¬å–æ¸¯è‚¡æ–°é—»çš„çˆ¬è™«æ¨¡å—ã€‚

## åŠŸèƒ½ç‰¹æ€§

- çˆ¬å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ¸¯è‚¡æ–°é—»ï¼ˆé»˜è®¤æœ€è¿‘1å¤©ï¼‰
- æ”¯æŒURLå’Œæ ‡é¢˜åŒé‡å»é‡
- è‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è¾“å‡º
- å¯é…ç½®çš„è¯·æ±‚å¤´å’Œå»¶è¿Ÿï¼Œé¿å…åçˆ¬è™«

## æ¨¡å—ç»“æ„

```
src/crawler/HKStocks/
â”œâ”€â”€ __init__.py           # æ¨¡å—åˆå§‹åŒ–ï¼Œå¯¼å‡ºä¸»è¦æ¥å£
â”œâ”€â”€ aastocks_scraper.py   # çˆ¬è™«æ ¸å¿ƒå®ç°
â”œâ”€â”€ models.py             # æ–°é—»æ•°æ®æ¨¡å‹
â”œâ”€â”€ utils.py              # å·¥å…·å‡½æ•°ï¼ˆæ—¥æœŸè§£æã€URLå¤„ç†ç­‰ï¼‰
â””â”€â”€ README.md             # æœ¬æ–‡æ¡£
```

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install beautifulsoup4 lxml requests
```

æˆ–ä½¿ç”¨é¡¹ç›®çš„ requirements.txt:

```bash
pip install -r requirements.txt
```

### 2. åŸºæœ¬ä½¿ç”¨

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•°ï¼ˆæ¨èï¼‰

```python
from src.crawler.HKStocks import scrape_hkstocks_news

# çˆ¬å–æœ€è¿‘1å¤©çš„æ–°é—»å¹¶ä¿å­˜åˆ°æ•°æ®åº“
news_list = scrape_hkstocks_news(days=1, save_to_db=True)

print(f"æˆåŠŸçˆ¬å– {len(news_list)} æ¡æ–°é—»")
```

#### æ–¹å¼äºŒï¼šä½¿ç”¨çˆ¬è™«ç±»

```python
from src.crawler.HKStocks import AaStocksScraper

# åˆ›å»ºçˆ¬è™«å®ä¾‹
scraper = AaStocksScraper()

# çˆ¬å–æ–°é—»
news_list = scraper.fetch_news(days=1)

# ä¿å­˜åˆ°æ•°æ®åº“
saved_count = scraper.save_to_database(news_list)
print(f"ä¿å­˜äº† {saved_count} æ¡æ–°é—»")
```

#### æ–¹å¼ä¸‰ï¼šè‡ªå®šä¹‰é…ç½®

```python
from src.crawler.HKStocks import AaStocksScraper
import config

# ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
scraper_config = {
    'base_url': config.HKSTOCKS_BASE_URL,
    'timeout': config.HKSTOCKS_REQUEST_TIMEOUT,
    'delay': config.HKSTOCKS_REQUEST_DELAY,
    'headers': config.HKSTOCKS_HEADERS
}

scraper = AaStocksScraper(config=scraper_config)
news_list = scraper.fetch_news(days=2)  # çˆ¬å–æœ€è¿‘2å¤©
```

### 3. è¿è¡Œæµ‹è¯•è„šæœ¬

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ
python test_hkstocks_crawler.py
```

æµ‹è¯•è„šæœ¬ä¼šï¼š
1. çˆ¬å–æœ€è¿‘1å¤©çš„æ–°é—»
2. æ˜¾ç¤ºå‰5æ¡æ–°é—»è¯¦æƒ…
3. è¯¢é—®æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“

## API æ–‡æ¡£

### `scrape_hkstocks_news()`

ä¾¿æ·å‡½æ•°ï¼Œä¸€é”®çˆ¬å–æ¸¯è‚¡æ–°é—»ã€‚

**å‚æ•°:**
- `days` (int): çˆ¬å–æœ€è¿‘å‡ å¤©çš„æ–°é—»ï¼Œé»˜è®¤1
- `config` (dict, optional): çˆ¬è™«é…ç½®å­—å…¸
- `save_to_db` (bool): æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“ï¼Œé»˜è®¤True

**è¿”å›:**
- `List[HKStockNews]`: æ–°é—»å¯¹è±¡åˆ—è¡¨

**ç¤ºä¾‹:**
```python
# çˆ¬å–æœ€è¿‘3å¤©çš„æ–°é—»
news = scrape_hkstocks_news(days=3)
```

---

### `AaStocksScraper` ç±»

AAStocks çˆ¬è™«æ ¸å¿ƒç±»ã€‚

#### åˆå§‹åŒ–

```python
scraper = AaStocksScraper(config=None)
```

**å‚æ•°:**
- `config` (dict, optional): é…ç½®å­—å…¸ï¼Œå¯åŒ…å«:
  - `base_url`: ç›®æ ‡ç½‘ç«™URL
  - `timeout`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
  - `delay`: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
  - `headers`: HTTPè¯·æ±‚å¤´

#### æ–¹æ³•

##### `fetch_news(days=1)`

çˆ¬å–æ–°é—»ã€‚

**å‚æ•°:**
- `days` (int): çˆ¬å–æœ€è¿‘å‡ å¤©çš„æ–°é—»

**è¿”å›:**
- `List[HKStockNews]`: æ–°é—»å¯¹è±¡åˆ—è¡¨

**å¼‚å¸¸:**
- `requests.RequestException`: ç½‘ç»œè¯·æ±‚å¤±è´¥

**ç¤ºä¾‹:**
```python
news_list = scraper.fetch_news(days=1)
```

##### `save_to_database(news_list, db_manager=None)`

ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“ï¼Œè‡ªåŠ¨å»é‡ã€‚

**å‚æ•°:**
- `news_list` (List[HKStockNews]): æ–°é—»åˆ—è¡¨
- `db_manager` (optional): æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹

**è¿”å›:**
- `int`: å®é™…ä¿å­˜çš„æ–°é—»æ•°é‡

**ç¤ºä¾‹:**
```python
saved_count = scraper.save_to_database(news_list)
```

---

### `HKStockNews` ç±»

æ–°é—»æ•°æ®æ¨¡å‹ã€‚

#### å±æ€§

- `title` (str): æ–°é—»æ ‡é¢˜
- `url` (str): æ–°é—»URL
- `content` (str): æ–°é—»æ­£æ–‡
- `publish_date` (datetime): å‘å¸ƒæ—¶é—´
- `source` (str): æ–°é—»æ¥æºï¼ˆé»˜è®¤"AAStocks"ï¼‰
- `category` (str, optional): æ–°é—»åˆ†ç±»

#### æ–¹æ³•

##### `to_dict()`

è½¬æ¢ä¸ºæ•°æ®åº“å…¼å®¹çš„å­—å…¸æ ¼å¼ã€‚

**è¿”å›:**
```python
{
    'channel_id': 'aastocks',
    'message_id': 123456789,  # URL+æ ‡é¢˜çš„å“ˆå¸Œå€¼
    'text': 'ã€æ ‡é¢˜ã€‘\n\næ­£æ–‡...',
    'date': '2025-11-05T14:30:00',
    'url': 'https://...',
    'title': 'æ ‡é¢˜'
}
```

---

## å·¥å…·å‡½æ•°

### `parse_chinese_date(date_str)`

è§£æAAStocksæ—¥æœŸæ ¼å¼ "2025/11/04 09:50 HKT"ã€‚

**å‚æ•°:**
- `date_str` (str): æ—¥æœŸå­—ç¬¦ä¸²

**è¿”å›:**
- `datetime`: è§£æåçš„æ—¥æœŸå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None

---

### `normalize_url(url, base_url)`

è§„èŒƒåŒ–URLï¼Œå¤„ç†ç›¸å¯¹è·¯å¾„ã€‚

---

### `is_within_days(date, days)`

åˆ¤æ–­æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šå¤©æ•°å†…ã€‚

---

### `generate_message_id(url, title)`

ç”Ÿæˆå”¯ä¸€çš„æ¶ˆæ¯IDç”¨äºå»é‡ã€‚

## é…ç½®è¯´æ˜

åœ¨ `config.py` ä¸­çš„é…ç½®é¡¹ï¼š

```python
# æ¸¯è‚¡æ–°é—»çˆ¬è™«é…ç½®
HKSTOCKS_SOURCE_ID = 'aastocks'
HKSTOCKS_BASE_URL = 'http://www.aastocks.com/tc/stocks/news/aafn'
HKSTOCKS_REQUEST_TIMEOUT = 30  # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
HKSTOCKS_REQUEST_DELAY = 1.5   # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
HKSTOCKS_HEADERS = {
    'User-Agent': '...',
    'Accept': '...',
    # ... å…¶ä»–è¯·æ±‚å¤´
}
```

## æ•°æ®åº“ç»“æ„

æ–°é—»å­˜å‚¨åœ¨ `testdb_history.db` çš„ `messages` è¡¨ä¸­ï¼š

```sql
CREATE TABLE messages (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    channel_id TEXT,      -- å›ºå®šä¸º 'aastocks'
    message_id INTEGER,   -- URL+æ ‡é¢˜çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºå»é‡ï¼‰
    text TEXT NOT NULL,   -- æ ¼å¼åŒ–åçš„æ–°é—»æ–‡æœ¬
    date TEXT NOT NULL    -- ISO 8601æ ¼å¼çš„æ—¥æœŸ
);
```

### å»é‡æœºåˆ¶

ä½¿ç”¨åŒé‡å»é‡ç­–ç•¥ï¼š
1. **message_id**: URLå’Œæ ‡é¢˜çš„å“ˆå¸Œå€¼
2. **text LIKE æ¨¡å¼**: æ£€æŸ¥æ ‡é¢˜æ˜¯å¦å·²å­˜åœ¨

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### 1. ç½‘ç»œè¿æ¥å¤±è´¥

```
é”™è¯¯: ç½‘ç»œè¯·æ±‚å¤±è´¥: Connection refused
```

**è§£å†³æ–¹æ¡ˆ:**
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ç¡®è®¤ç½‘ç«™æ˜¯å¦å¯è®¿é—®
- æ£€æŸ¥æ˜¯å¦éœ€è¦ä»£ç†

#### 2. è¯·æ±‚è¶…æ—¶

```
é”™è¯¯: è¯·æ±‚è¶…æ—¶: http://...
```

**è§£å†³æ–¹æ¡ˆ:**
- å¢åŠ è¶…æ—¶æ—¶é—´: `config['timeout'] = 60`
- æ£€æŸ¥ç½‘ç»œé€Ÿåº¦

#### 3. HTTPé”™è¯¯

```
é”™è¯¯: HTTPé”™è¯¯ 403: http://...
```

**è§£å†³æ–¹æ¡ˆ:**
- ç½‘ç«™å¯èƒ½æœ‰åçˆ¬è™«æœºåˆ¶
- å°è¯•æ›´æ¢User-Agent
- å¢åŠ è¯·æ±‚å»¶è¿Ÿ

#### 4. è§£æå¤±è´¥

```
æ— æ³•è§£ææ—¥æœŸæ ¼å¼: ...
```

**è§£å†³æ–¹æ¡ˆ:**
- ç½‘ç«™HTMLç»“æ„å¯èƒ½å˜åŒ–
- éœ€è¦æ›´æ–° `_parse_news_list()` æ–¹æ³•
- æŸ¥çœ‹ç½‘é¡µæºç ï¼Œè°ƒæ•´CSSé€‰æ‹©å™¨

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æ–°é—»æº

1. åœ¨ `src/crawler/` ä¸‹åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼Œå¦‚ `USStocks/`
2. å‚è€ƒ `HKStocks/` çš„ç»“æ„åˆ›å»ºæ–‡ä»¶
3. ä¿®æ”¹ `base_url` å’Œè§£æé€»è¾‘
4. ç¡®ä¿ `to_dict()` è¿”å›ç›¸åŒæ ¼å¼

### è‡ªå®šä¹‰è§£æé€»è¾‘

å¦‚æœç½‘ç«™ç»“æ„å˜åŒ–ï¼Œéœ€è¦ä¿®æ”¹ `aastocks_scraper.py` ä¸­çš„ï¼š

1. `_parse_news_list()` - æ–°é—»åˆ—è¡¨è§£æ
2. `_fetch_news_detail()` - æ–°é—»è¯¦æƒ…è§£æ

å‚è€ƒBeautifulSoupæ–‡æ¡£ï¼šhttps://www.crummy.com/software/BeautifulSoup/

## æ³¨æ„äº‹é¡¹

1. **éµå®ˆç½‘ç«™æ¡æ¬¾**: çˆ¬å–å‰è¯·é˜…è¯»ç½‘ç«™çš„robots.txtå’Œä½¿ç”¨æ¡æ¬¾
2. **é€‚å½“å»¶è¿Ÿ**: é¿å…é¢‘ç¹è¯·æ±‚ï¼Œå»ºè®®è‡³å°‘1ç§’å»¶è¿Ÿ
3. **æ•°æ®éšç§**: ä¸è¦çˆ¬å–ç”¨æˆ·éšç§ä¿¡æ¯
4. **é”™è¯¯å¤„ç†**: ç”Ÿäº§ç¯å¢ƒå»ºè®®æ·»åŠ é‡è¯•æœºåˆ¶å’Œå‘Šè­¦
5. **å®šæœŸç»´æŠ¤**: ç½‘ç«™ç»“æ„å¯èƒ½å˜åŒ–ï¼Œéœ€è¦å®šæœŸæ£€æŸ¥å’Œæ›´æ–°

## ç¤ºä¾‹ä»£ç 

### å®šæ—¶çˆ¬å–

```python
import schedule
import time
from src.crawler.HKStocks import scrape_hkstocks_news

def job():
    """å®šæ—¶ä»»åŠ¡"""
    print("å¼€å§‹çˆ¬å–æ¸¯è‚¡æ–°é—»...")
    news = scrape_hkstocks_news(days=1)
    print(f"å®Œæˆï¼çˆ¬å–äº† {len(news)} æ¡æ–°é—»")

# æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
schedule.every().hour.do(job)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### çˆ¬å–å¹¶åˆ†æ

```python
from src.crawler.HKStocks import scrape_hkstocks_news
from src.keyword_extraction.keyword_extractor import get_keyword_extractor

# çˆ¬å–æ–°é—»
news_list = scrape_hkstocks_news(days=1)

# æå–å…³é”®è¯
extractor = get_keyword_extractor()

for news in news_list:
    keywords = extractor.extract_keywords(news.content)
    print(f"{news.title}")
    print(f"å…³é”®è¯: {', '.join([kw['keyword'] for kw in keywords[:5]])}")
    print()
```

## è®¸å¯è¯

æœ¬æ¨¡å—ä¸ºå­¦æœ¯é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

## å…³é”®è¯æå–é›†æˆ (NEW)

### åŠŸèƒ½è¯´æ˜

ä» v1.1.0 å¼€å§‹ï¼Œçˆ¬è™«é›†æˆäº†è‡ªåŠ¨å…³é”®è¯æå–åŠŸèƒ½ã€‚çˆ¬å–æ–°é—»åä¼šè‡ªåŠ¨æå–å…³é”®è¯å¹¶ä¿å­˜åˆ°æ•°æ®åº“çš„ `keywords` å­—æ®µã€‚

### ä½¿ç”¨æ–¹æ³•

#### 1. é»˜è®¤å¯ç”¨å…³é”®è¯æå–

```python
from src.crawler.HKStocks import scrape_hkstocks_news

# çˆ¬å–æ–°é—»ï¼Œè‡ªåŠ¨æå–å…³é”®è¯ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
news_list = scrape_hkstocks_news(days=1, save_to_db=True)
```

#### 2. ç¦ç”¨å…³é”®è¯æå–

```python
from src.crawler.HKStocks import AaStocksScraper

scraper = AaStocksScraper()
news_list = scraper.fetch_news(days=1)

# ä¿å­˜æ—¶ç¦ç”¨å…³é”®è¯æå–
scraper.save_to_database(news_list, extract_keywords=False)
```

### å…³é”®è¯æå–æŠ€æœ¯

- **æ¨¡å‹**: KeyBERT (paraphrase-multilingual-MiniLM-L12-v2)
- **åˆ†è¯**: jieba ä¸­æ–‡åˆ†è¯
- **åœç”¨è¯**: 1201ä¸ªï¼ˆåŒ…å«é‡‘èé¢†åŸŸæœ¯è¯­ï¼‰
- **æå–æ•°é‡**: æ¯æ¡æ–°é—» 10 ä¸ªå…³é”®è¯
- **è¾“å…¥**: æ–°é—»æ ‡é¢˜ + å†…å®¹

è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹: [src/hkstocks_analysis/README.md](../../hkstocks_analysis/README.md)

### æ•°æ®åº“è¡¨ç»“æ„æ›´æ–°

`hkstocks_news` è¡¨æ–°å¢ `keywords` å­—æ®µï¼š

```sql
CREATE TABLE hkstocks_news (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,
    url TEXT NOT NULL UNIQUE,
    content TEXT NOT NULL,
    publish_date TEXT NOT NULL,
    source TEXT DEFAULT 'AAStocks',
    category TEXT,
    keywords TEXT,                          -- æ–°å¢ï¼šå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now'))
);
```

### å…³é”®è¯æ ¼å¼

å…³é”®è¯ä»¥é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²å­˜å‚¨ï¼š

```
è…¾è®¯æ§è‚¡,ç¬¬ä¸‰å­£åº¦,å‡€åˆ©æ¶¦,ä¸šç»©æŠ¥å‘Š,é‡‘èç§‘æŠ€,ä¼ä¸šæœåŠ¡,æ¸¸æˆä¸šåŠ¡,äººå·¥æ™ºèƒ½,äº‘è®¡ç®—,åŒæ¯”å¢é•¿
```

è§£æç¤ºä¾‹ï¼š

```python
import sqlite3

conn = sqlite3.connect('data/news_analysis.db')
cursor = conn.cursor()

cursor.execute("""
    SELECT title, keywords
    FROM hkstocks_news
    WHERE keywords IS NOT NULL
    LIMIT 1
""")

title, keywords_str = cursor.fetchone()
keywords_list = keywords_str.split(',')

print(f"æ–°é—»: {title}")
print(f"å…³é”®è¯: {keywords_list}")
```

### æµ‹è¯•

è¿è¡Œé›†æˆæµ‹è¯•ï¼š

```bash
# æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„
python test_crawler_with_keywords.py --check-schema

# å®Œæ•´æµ‹è¯•ï¼ˆçˆ¬å–å¹¶æå–å…³é”®è¯ï¼‰
python test_crawler_with_keywords.py
```

### ä¾èµ–è¦æ±‚

å…³é”®è¯æå–éœ€è¦é¢å¤–çš„ä¾èµ–ï¼š

```bash
pip install keybert sentence-transformers spacy jieba
python -m spacy download zh_core_web_sm
```

å¦‚æœç¼ºå°‘ä¾èµ–ï¼Œçˆ¬è™«ä¼šè‡ªåŠ¨é™çº§ä¸ºä¸æå–å…³é”®è¯æ¨¡å¼ã€‚

### ç¤ºä¾‹è¾“å‡º

```
å¼€å§‹çˆ¬å–AAStocksæ–°é—» (æœ€è¿‘ 1 å¤©)...
æ±‡æ€»é¡µé¢æ‰¾åˆ° 45 æ¡æ–°é—»é“¾æ¥
åˆå§‹åŒ–å…³é”®è¯æå–å™¨...
Loading KeyBERT model...
å…³é”®è¯æå–å™¨å·²å°±ç»ª

  âœ“ [2025-11-12 14:30] é¨°è¨Šæ§è‚¡å…¬å¸ƒç¬¬ä¸‰å­£åº¦æ¥­ç¸¾...
  + ä¿å­˜æ–°é—»: é¨°è¨Šæ§è‚¡å…¬å¸ƒç¬¬ä¸‰å­£åº¦æ¥­ç¸¾...
    å…³é”®è¯: è…¾è®¯æ§è‚¡,ç¬¬ä¸‰å­£åº¦,å‡€åˆ©æ¶¦,ä¸šç»©æŠ¥å‘Š,é‡‘èç§‘æŠ€...

ä¿å­˜å®Œæˆ: æ–°å¢ 5 æ¡ï¼Œæ›´æ–° 0 æ¡ï¼Œè·³è¿‡é‡å¤ 0 æ¡
```

## æ›´æ–°æ—¥å¿—

### v1.1.0 (2025-11-12)
- âœ¨ æ–°å¢è‡ªåŠ¨å…³é”®è¯æå–åŠŸèƒ½
- âœ¨ æ•°æ®åº“è¡¨æ–°å¢ keywords å­—æ®µ
- âœ¨ é›†æˆ KeyBERT + jieba å…³é”®è¯æå–
- ğŸ”§ æ”¯æŒç¦ç”¨å…³é”®è¯æå–é€‰é¡¹
- ğŸ“ æ›´æ–°æ–‡æ¡£å’Œæµ‹è¯•è„šæœ¬

### v1.0.0 (2025-11-05)
- åˆå§‹ç‰ˆæœ¬
- æ”¯æŒAAStocksæ–°é—»çˆ¬å–
- å®ç°URL+æ ‡é¢˜åŒé‡å»é‡
- æ·»åŠ å®Œæ•´çš„é”™è¯¯å¤„ç†
